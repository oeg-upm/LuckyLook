{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 5. Redes Neuronales Convolucionales</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>5. Análisis opiniones de películas</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Conjunto de datos](#section1)\n",
    "    * [1.1. Cargar el dataset IMDB](#section1.1)\n",
    "* [2. Incrustar palabras](#section2)\n",
    "* [3. Multilayer Perceptron](#section3)\n",
    "    * [3.1. Librería y dataset](#section3.1)\n",
    "    * [3.2. Extraer número de reseñas](#section3.2)\n",
    "    * [3.3. Desarrollar MLP](#section3.3)\n",
    "    * [3.4. Evaluación del modelo](#section3.4)\n",
    "    * [2.5. Evaluación del modelo](#section3.5)\n",
    "* [4. CNN Unidimensional](#section4)\n",
    "    * [4.1. Librería y dataset](#section4.1)\n",
    "    * [4.2. Desarrollar la CNN](#section4.2)\n",
    "    * [4.3. Evaluación del modelo](#section4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de sentimientos es un problema de procesamiento del lenguaje natural donde se comprende el texto y se predice la intención subyacente. En esta lección, descubrirá cómo puede predecir el sentimiento de las críticas de películas como positivo o negativo en Python utilizando la biblioteca de Deep Learning de Keras. Después de completar este tutorial paso a paso, sabrá:\n",
    "* Sobre el problema de análisis de sentimientos de IMDB para el procesamiento del lenguaje natural y cómo cargarlo en Keras.\n",
    "* Cómo usar la inserción de palabras en Keras para problemas de lenguaje natural.\n",
    "* Cómo desarrollar y evaluar un modelo de perceptrón multicapa para el problema de IMDB.\n",
    "* Cómo desarrollar un modelo de red neuronal convolucional unidimensional para el problema IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Conjunto de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos utilizado en este proyecto es IMDB. El conjunto de datos de IMDB contiene 50.000 reseñas de películas (buenas o malas) para entrenamiento y la misma cantidad nuevamente para test. El problema es determinar si una crítica de película determinada tiene un sentimiento positivo o negativo.\n",
    "\n",
    "Los datos fueron recopilados por investigadores de Stanford y se utilizaron en un artículo de 2011 en el que se utilizó una división del 50-50 de los datos para el entrenamiento y la prueba. Se logró una precisión del 88,89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el dataset [IMBD](http://ai.stanford.edu/~amaas/data/sentiment/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el artículo original [_Learning Word Vectors for Sentiment Analysis_](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.1. Cargar el dataset IMDB</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras proporciona acceso al conjunto de datos de IMDB integrado. La función `imdb.load data()` permite cargar el conjunto de datos en un formato que está listo para usar en modelos. Las palabras han sido reemplazadas por números enteros que indican la popularidad absoluta de la palabra en el conjunto de datos. Por lo tanto, las oraciones de cada revisión se componen de una secuencia de números enteros.\n",
    "\n",
    "Llamar a `imdb.load data()` por primera vez descargará el conjunto de datos IMDB a su computadora y lo almacenará en su directorio personal en `~/.keras/datasets/imdb.pkl` como un archivo de 32 megabytes. \n",
    "\n",
    "De manera útil, la función `imdb.load data()` proporciona argumentos adicionales, como:\n",
    "1. El número de palabras principales para cargar (donde las palabras con un número entero inferior se marcan como cero en los datos devueltos)\n",
    "2. El número de palabras principales para omitir (como _\"the's\")_ \n",
    "3. La duración máxima de las revisiones para respaldar. \n",
    "\n",
    "Carguemos el conjunto de datos y calculemos algunas de sus propiedades. Comenzaremos cargando algunas bibliotecas y cargando todo el conjunto de datos de IMDB como un conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el dataset IMDB de [Keras](https://keras.io/api/datasets/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Plot the IMDB dataset\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, podemos mostrar la forma del conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos imprimir los valores de clase únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, podemos tener una idea del número total de palabras únicas en el conjunto de datos. Curiosamente, podemos ver que hay poco menos de 100.000 palabras en todo el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos hacernos una idea de la longitud media de las reseñas. Podemos ver que la reseña promedio tiene poco menos de 300 palabras con una desviación estándar de poco más de 200 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length as a boxplot and histogram\n",
    "plt.subplot(121)\n",
    "plt.boxplot(result)\n",
    "plt.subplot(122)\n",
    "plt.hist(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirando el BoxPlot y el histograma para las longitudes de revisión en palabras, probablemente podamos ver una distribución exponencial que probablemente podamos cubrir la masa de la distribución con una longitud recortada de 400 a 500 palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Incrustar palabras</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un avance reciente en el campo del procesamiento del lenguaje natural se llama incrustación de palabras _(word embeddings)._ Se trata de una técnica en la que las palabras se codifican como vectores de valor real en un espacio de alta dimensión, donde la similitud entre palabras en términos de significado se traduce en cercanía en el espacio vectorial. Las palabras discretas se asignan a vectores de números continuos. Esto es útil cuando se trabaja con problemas de lenguaje natural con redes neuronales, ya que necesitamos números como valores de entrada.\n",
    "\n",
    "Keras proporciona una forma conveniente de convertir representaciones enteras positivas de palabras en una incrustación de palabras mediante una capa de incrustación 4. La capa toma argumentos que definen el mapeo, incluido el número máximo de palabras esperadas, también llamado tamaño de vocabulario (por ejemplo, el valor entero más grande que se verá como entrada). La capa también le permite especificar la dimensionalidad de cada vector de palabra, denominada dimensión de salida.\n",
    "\n",
    "Nos gustaría utilizar una representación de incrustación de palabras para el conjunto de datos de IMDB. Supongamos que solo nos interesan las primeras 5,000 palabras más utilizadas en el conjunto de datos. Por lo tanto, nuestro tamaño de vocabulario será de 5,000. Podemos optar por utilizar un vector de 32 dimensiones para representar cada palabra. Finalmente, podemos optar por limitar la extensión máxima de la reseña a 500 palabras, truncando las reseñas más largas y rellenando las reseñas más cortas con valores 0. Cargaríamos el conjunto de datos de IMDB de la siguiente manera:\n",
    "\n",
    "```python\n",
    "    imdb.load_data(num_words=5000)\n",
    "```\n",
    "\n",
    "Luego, usaríamos la utilidad Keras para truncar o rellenar el conjunto de datos a una longitud de 500 para cada observación usando la función `sequence.pad_sequences()`.\n",
    "\n",
    "```python\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "```\n",
    "\n",
    "Finalmente, más adelante, la primera capa de nuestro modelo sería una capa de incrustación de palabras creada usando la clase `Embedding` de la siguiente manera:\n",
    "\n",
    "```python\n",
    "    Embedding(5000, 32, input_length=500)\n",
    "```\n",
    "\n",
    "La salida de esta primera capa sería una matriz con el tamaño de $32 × 500$ para un entrenamiento de revisión de película determinado o un patrón de prueba en formato entero. Ahora que sabemos cómo cargar el conjunto de datos de IMDB en Keras y cómo usar una representación de incrustación de palabras para ello, desarrollemos y evaluemos algunos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre la clase [Embedding](http://keras.io/layers/embeddings/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Multilayer Perceptron</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comenzar desarrollando un modelo MLP con una sola capa oculta. La palabra incrustación de representación es una verdadera innovación y demostraremos lo que se habría considerado resultados de clase mundial en 2011 con una red neuronal relativamente simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.1. Librería y dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos importando las clases y funciones necesarias para este modelo.\n",
    "\n",
    "A continuación, cargaremos el conjunto de datos IMDB. Simplificaremos el conjunto de datos como se discutió durante la sección sobre incrustaciones de palabras. Solo se cargarán las 5.000 palabras principales. También usaremos una división del 50% / 50% del conjunto de datos en entrenamiento y prueba. Ésta es una buena metodología de división estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.2. Extraer número de reseñas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitaremos las reseñas a 500 palabras, truncando las reseñas más largas y eliminando las reseñas más cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.3. Desarrollar MLP</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear nuestro modelo. Usaremos\n",
    "1. Una capa de incrustación como capa de entrada, estableciendo el vocabulario en 5,000, el tamaño del vector de la palabra en 32 dimensiones y la longitud de entrada en 500. \n",
    "2. La salida de esta primera capa será una matriz de tamaño $32 × 500$ como se discutió en la sección anterior. sección.\n",
    "3. Aplanaremos la salida de las capas de incrustación a una dimensión.\n",
    "4. Luego usaremos una capa oculta densa de 250 unidades con una función de activación ReLu. \n",
    "5. La capa de salida tiene una neurona y utilizará una activación sigmoidea para generar valores de 0 y 1 como predicciones. \n",
    "6. El modelo utiliza pérdida logarítmica y se optimiza mediante ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.4\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.4. Evaluación del modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ajustar el modelo y usar el conjunto de pruebas como validación durante el entrenamiento. Este modelo se adapta muy rápidamente, por lo que usaremos muy pocas épocas de entrenamiento, en este caso solo 2. \n",
    "\n",
    "Hay muchos datos, por lo que usaremos un tamaño de batch de 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 8s - loss: 0.4839 - accuracy: 0.7421 - val_loss: 0.2946 - val_accuracy: 0.8754\n",
      "Epoch 2/2\n",
      "196/196 - 8s - loss: 0.1802 - accuracy: 0.9313 - val_loss: 0.3124 - val_accuracy: 0.8708\n",
      "Accuracy: 87.08%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que este modelo muy simple alcanza una puntuación cercana al 87,23% que se encuentra en la vecindad del artículo original, con muy poco esfuerzo.\n",
    "\n",
    "Estoy seguro de que podemos hacerlo mejor si entrenamos esta red, quizás usando una incorporación más grande y agregando más capas ocultas. Probemos con un tipo de red diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. CNN Unidimensional </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales convolucionales se diseñaron para honrar la estructura espacial en los datos de la imagen y al mismo tiempo ser robustas a la posición y orientación de los objetos aprendidos en la escena. Este mismo principio se puede utilizar en secuencias, como la secuencia unidimensional de palabras en una reseña de películas.\n",
    "\n",
    "Las mismas propiedades que hacen atractivo el modelo de CNN para aprender a reconocer objetos en imágenes pueden ayudar a aprender la estructura en párrafos de palabras, es decir, las técnicas de invarianza a la posición específica de las características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.1. Librería y dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras admite convoluciones unidimensionales y agrupación mediante las clases `Conv1D` y `MaxPooling1D`, respectivamente. Nuevamente, importemos las clases y funciones necesarias, además del dataset IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.2. Desarrollar la CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir nuestro modelo de red neuronal convolucional. Esta vez:\n",
    "\n",
    "1. Después de la capa de entrada Embedding, insertamos una capa **Conv1D.** \n",
    "    * Esta capa convolucional tiene 32 mapas de características y lee representaciones de palabras incrustadas 3 elementos vectoriales de la palabra incrustada a la vez. \n",
    "2. La capa convolucional va seguida de una capa **MaxPooling1D** con una longitud y un paso de 2 que divide a la mitad el tamaño de los mapas de características de la capa convolucional. \n",
    "3. El resto de la red es igual que la red neuronal anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>4.3. Evaluación del modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nuestra capa convolucional conserva la dimensionalidad de nuestra capa de entrada de incrustación de entrada de 32 dimensiones con un máximo de 500 palabras. La capa de agrupación comprime esta representación dividiéndola a la mitad. La ejecución del ejemplo ofrece una mejora pequeña sobre el modelo de red neuronal anterior con una precisión de casi el 88,66%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 11s - loss: 0.4472 - accuracy: 0.7653 - val_loss: 0.2707 - val_accuracy: 0.8869\n",
      "Epoch 2/2\n",
      "196/196 - 11s - loss: 0.2118 - accuracy: 0.9180 - val_loss: 0.2702 - val_accuracy: 0.8869\n",
      "Accuracy: 88.69%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, hay muchas oportunidades para una mayor optimización, como el uso de capas convolucionales más profundas y / o más grandes. Una idea interesante es configurar la capa de agrupación máxima para usar una longitud de entrada de 500. Esto comprimiría cada mapa de características en un solo vector de 32 longitudes y podría aumentar el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
